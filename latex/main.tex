\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{setspace}
\usepackage{caption}
\captionsetup[table]{width=\textwidth}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{titletoc}
\usepackage{amsfonts}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage[spanish]{babel}
\addto\captionsspanish{\renewcommand{\figurename}{Figura}}
\usepackage{hyperref}
\hypersetup{
    pdfborder={0 0 0}
}
\usepackage[spanish]{babel}
\def\figureautorefname{Figura}
\setcounter{secnumdepth}{3}
\usepackage[style=ieee, backend=biber]{biblatex}
\addbibresource{referencias.bib}
\DefineBibliographyStrings{spanish}{
    url = {Obtenido de:},
    urlseen = {Consultado el},
}
\usepackage{url}
\usepackage{xcolor}
\newcommand{\headlinecolor}{\color{black}}


% Márgenes y espaciado
\geometry{left=3cm, right=2.5cm, top=3cm, bottom=2.5cm}
\newcommand{\hsp}{\hspace{-5pt}}
% Número a la derecha de cada capítulo y espaciado corregido
\titleformat{\chapter}[hang]
  {\vspace{-1cm}
  \headlinecolor\Large\bfseries} % formato
  {\thechapter.\hsp}             % número
  {10pt}                         % separación número - título
  {\Large\bfseries}               % formato del título

% Espaciados capítulos: {izquierda}{antes}{después}
\titlespacing*{\chapter}{0pt}{-8pt}{16pt}

% Espaciados secciones
\titlespacing*{\section}{0pt}{14pt}{8pt}
\titlespacing*{\subsection}{0pt}{10pt}{6pt}

% Para que todo el documento tenga el tipo de letra Arial
\usepackage{fontspec}

% Selecciona Arial como fuente principal
\setmainfont{Arial}

\begin{document}

\renewcommand{\figurename}{Figura}

\pagenumbering{gobble} % ELIMINA NUMERACION EN PORTADA
\begin{titlepage}
\renewcommand{\baselinestretch}{1.4}\normalsize

\newgeometry{left=3cm,right=3cm,top=3cm,bottom=3cm}
\centering

{\bfseries\fontsize{24}{24}\selectfont ESCUELA POLITÉCNICA NACIONAL\par}\vspace{1.5cm}

{\bfseries\fontsize{16}{20}\selectfont FACULTAD DE INGENIERÍA EN SISTEMAS\par}\vspace{1.5cm}

{\bfseries\fontsize{14}{17}\selectfont OPTIMIZACIÓN DE SISTEMAS DE INFORMACIÓN EN CONTEXTOS EMPRESARIALES\par}\vspace{1cm}

{\bfseries\fontsize{14}{17}\selectfont ANÁLISIS Y SEGMENTACIÓN DE CLIENTES NO REGULADOS DEL SECTOR ELÉCTRICO MEDIANTE ALGORITMOS DE APRENDIZAJE NO SUPERVISADO\par}\vspace{1.5cm}

{\bfseries\fontsize{12}{14}\selectfont TRABAJO DE INTEGRACIÓN CURRICULAR PRESENTADO COMO REQUISITO PARA LA OBTENCIÓN DEL TÍTULO DE INGENIERO EN CIENCIAS DE LA COMPUTACIÓN\par}\vspace{1.5cm}

{\bfseries\fontsize{12}{14}\selectfont ANDRÉS ANTONIO ZAMBRANO ALQUINGA\par}
{\fontsize{12}{14}\selectfont andres.zambrano03@epn.edu.ec\par}\vspace{1cm}

{\bfseries\fontsize{12}{14}\selectfont DIRECTOR: JOSAFÁ DE JESÚS AGUIAR PONTES\par}
{\fontsize{12}{14}\selectfont josafa.aguiar@epn.edu.ec\par}\vspace{1.5cm}

{\bfseries\fontsize{12}{14}\selectfont DMQ, julio 2025\par}
    
\restoregeometry
\end{titlepage}


%% CERTIFICACIONES

\renewcommand{\baselinestretch}{1.25}\normalsize

\pagenumbering{roman} % NUMERACION EN ROMANO
\chapter*{Certificaciones}
\addcontentsline{toc}{chapter}{Certificaciones}

Yo, \textbf{Andrés Zambrano}, declaro que el trabajo de integración curricular aquí descrito es de mi autoría; que no ha sido previamente presentado para ningún grado o calificación profesional; y, que he consultado las referencias bibliográficas que se incluyen en este documento.\\[2cm]
\textbf{NOMBRE\_ESTUDIANTE}\\[1cm]

Certifico que el presente trabajo de integración curricular fue desarrollado por Andrés Zambrano, bajo mi supervisión.\\[2cm]

\textbf{NOMBRE\_DIRECTOR}\\ \textbf{DIRECTOR}



%% DECLARATORIA DE AUTORIA



\chapter*{Declaración de autoría}
\addcontentsline{toc}{chapter}{Declaración de autoría}

A través de la presente declaración, afirmamos que el trabajo de integración curricular aquí descrito, así como el (los) producto(s) resultante(s) del mismo, son públicos y estarán a disposición de la comunidad a través del repositorio institucional de la Escuela Politécnica Nacional; sin embargo, la titularidad de los derechos patrimoniales nos corresponde a los autores que hemos contribuido en el desarrollo del presente trabajo; observando para el efecto las disposiciones establecidas por el órgano competente en propiedad intelectual, la normativa interna y demás normas.\\[2cm]
\textbf{Andrés Zambrano}\\ \textbf{Josafá Aguiar}


% DEDICATORIA


\chapter*{Dedicatoria}
\addcontentsline{toc}{chapter}{Dedicatoria}

A mis padres celestiales, Dios y la santísima Virgen María, en quienes siempre he depositado toda mi fé y confianza a lo largo de toda mi trayectoria académica.\\

\vspace{-2mm}

A mis padres, Verito y Marco, quienes a pesar de todas las dificultades que se presentaron a lo largo del camino, nunca dudaron de mí, y en su lugar, siempre supieron alentarme y darme su apoyo incondicional para seguir adelante, sin lugar a dudas, este, y todos mis logros se los dedico a ustedes. \\

\vspace{-2mm}

A Edita Vélez, mi segunda mamá, quien me cuidó durante toda mi niñez, llenándome siempre de amor, mimos y mucho cariño.\\

\vspace{-2mm}

A mis padrinos, Franklin Vásquez y Silvana Barba, por acogerme con cariño en su hogar durante mis estudios universitarios, de igual manera, a mis primos, Carolina, Dennis y Pamela, quienes más que primos han sido como hermanos para mí.\\

\vspace{-2mm}

A Jhonny Sánchez, mi hermano de otra madre, con quien he compartido invaluables momentos durante gran parte de mi niñez. Gracias por ser ese hermano que nunca pude tener, pero que la vida se encargó de darme.\\

\vspace{-2mm}

A la memoria de mis abuelitos, Teresa y Manuel, quienes a pesar de ya no estar físicamente conmigo, sigo sintiendo su amor y protección en cada paso que doy.\\

\vspace{-2mm}

A mis amigos, compañeros de risas, retos e innumerables experiencias, que siempre han estado presentes, tanto en las buenas como en las malas. \\

\vspace{-2mm}

A toda mi familia en general, quienes de manera directa o indirecta han contribuido con su granito de arena para formar la persona que soy hoy en día.\\

\vspace{-2mm}

Finalmente, a mis dos peluditos, Rockie y Merlín, especialmente a mi gordo, Merlín, mi más linda compañía durante mi transición por propedéutico, pasó largas noches de vela a mi lado brindándome de su cálida compañía mientras yo estudiaba.




% AGRADECIMIENTOS




\chapter*{Agradecimientos}
\addcontentsline{toc}{chapter}{Agradecimientos}
Agradezco en primer lugar, a Dios y a la Vírgen María por no desampararme nunca en ninguna etapa de mi vida, por haberme guiado en cada momento, y por empaparme de sabiduría durante toda mi transición por la universidad.\\

\vspace{-2mm}

A mis padres, mis dos grandes tesoros, gracias por creer en mí en todo momento, por demostrarme que con esfuerzo y dedicación todo es posible y, sobre todo, por su amor y apoyo incondicional. Gracias por tanto, gracias por ser mis padres.\\

\vspace{-2mm}

Quiero agradecer de manera muy especial a mi prima Carolina Vásquez por todo lo que ha hecho por mí. Gracias Carito por ser una guía indispensable y un apoyo incondicional en mi vida, eres como una hermana para mí.\\

\vspace{-2mm}

Agradezco de igual manera al ingeniero Boris Astudillo por compartir conmigo sus valiosos consejos durante mi vida universitaria y, por su constante guía y apoyo durante todo el proceso de desarrollo de mi proyecto de titulación.\\

\vspace{-2mm}

A mi alma máter, la Escuela Politécnica Nacional y a los docentes que contribuyeron a mi formación académica, por brindarme todos los conocimientos y las herramientas necesarias para desarrollarme como profesional.\\

\vspace{-2mm}

Quiero agradecer a todo el equipo de la Empresa Eléctrica Quito, por su apoyo y guía durante el desarrollo de mis prácticas preprofesionales, en especial a los ingenieros e ingenieras Carolina, William, Oscar, Claudia, Isabel y Grace. Agradezco de igual manera al ingeniero Ricardo Dávila por brindarme la confianza y la oportunidad de vivir esta experiencia invaluable para mi desarrollo profesional.\\

\vspace{-2mm}

Finalmente, agradezco a mis amigos Carlos, Alexis, Hernán, Galo, Dilan y los que faltan por nombrar, por hacer que la vida universitaria fuera mucho más llevadera. Gracias por todas las experiencias que compartimos, risas, enojos, tristezas, largas charlas, y sobre todo, la remontada del siglo en sexto semestre.


% ÍNDICE DE CONTENIDOS


\tableofcontents


% RESUMEN


\chapter*{Resumen}

Este Trabajo de Integración Curricular aborda un proyecto de minería de datos enfocado en la implementación de un algoritmo de aprendizaje no supervisado para segmentar clientes en grupos homogéneos a partir de sus curvas características de consumo anual. El objetivo es identificar patrones de consumo energético que permitan una planificación más eficiente y una optimización del uso de la energía en el sector eléctrico.\\

La metodología aplicada es CRISP-DM, con una modificación en su fase final. Dentro de la misma, se han planteado dos procesos claves a seguir: en primer lugar, se desarrolla un proceso ETL orquestado por Apache Airflow, para la consolidación y transformación de los datos mensuales en una curva característica representativa anual por cada cliente, posteriormente, en el proceso de agrupación, se seleccionan y optimizan varios algoritmos para agrupar a los clientes en base a la similitud de sus curvas de consumo.\\

Los resultados de cada algoritmo son evaluados mediante diversas métricas, que cuantifican la calidad de las agrupaciones, con el fin de determinar el algoritmo que ofrece las agrupaciones de mejor calidad. Los resultados de agrupación serán presentados de manera visual y cuantitativa.\\


\textbf{Palabras clave:} minería de datos, segmentación de clientes, curvas de consumo, aprendizaje no supervisado, algoritmos de clustering, planificación energética, proceso ETL, Apache Airflow, CRISP-DM.


% ABSTRACT


\chapter*{Abstract}

This Curriculum Integration Project focuses on a data mining project aimed at implementing an unsupervised learning algorithm to segment clients into homogeneous groups based on their annual characteristic consumption curves. The goal is to identify energy consumption patterns that allow for more efficient planning and optimization of energy use in the electric sector.\\

The methodology applied is CRISP-DM, with a modification in its final phase. Within this framework, two key processes are followed: first, an ETL process orchestrated by Apache Airflow is developed to consolidate and transform monthly data into an annual representative characteristic curve for each client. Then, in the grouping process, several algorithms are selected and optimized to group clients based on the similarity of their consumption curves.\\

The results of each algorithm are evaluated using various metrics that quantify the quality of the groupings, in order to determine which algorithm provides the highest-quality groupings. The grouping results will be presented both visually and quantitatively.\\

\textbf{Keywords:} data mining, customer segmentation, consumption curves, unsupervised learning, clustering algorithms, energy planning, ETL process, Apache Airflow, CRISP-DM.

\newpage
\pagenumbering{arabic} % NUMERACION NORMAL

% INTRODUCCIÓN

\chapter{DESCRIPCIÓN DEL COMPONENTE DESARROLLADO}


En el contexto actual de las empresas distribuidoras de energía, como la Empresa Eléctrica Quito (EEQ), la eficiente gestión energética es uno de los principales desafíos a enfrentar. Diversos factores como la diversificación en los hábitos de consumo y variablidad de la demanda dificultan la planificación y diseño de estrategias eficientes que permitan responder de manera adecuada. Los métodos tradicionales de análisis, que se basan en promedios o clasificaciones rígidas resultan insuficientes para capturar dicha complejidad en los patrones de consumo de los clientes, dificultando el diseño de una planificación energética eficiente.\\

El análisis del consumo energético es un aspecto fundamental para la optimización de recursos en sectores como la distribución eléctrica y la gestión de tarifas, debido a esto, identificar patrones de consumo permite segmentar a los clientes en función de su comportamiento energético, lo cual facilita la toma de decisiones estratégicas, garantizando así una planificación energética eficiente.\\

Ante esta problemática, se ha desarrollado un componente orientado a la segmentación inteligente de clientes, implementando un proyecto de minería de datos que propone un enfoque basado en técnicas de aprendizaje no supervisado para la segmentación de clientes en función de la forma de su curva característica anual de consumo energético. El objetivo principal es identificar patrones de consumo que permitan una planificación más eficiente y optimización del uso de la energía en el sector eléctrico.\\

Bajo este contexto, el desarrollo del componente es realizado bajo la metodología CRISP-DM, con una ligera modificación en su fase final. Mientras que en la metodología original la fase final se centra en la implementación y despliegue del modelo, en este caso, el objetivo final es, entre todas las agrupaciones dadas por los diferentes algoritmos, escoger aquella que tenga la mejor calidad y homogeneidad, basándose en métricas de evaluación. Esta modificación de la fase final es posible debido a que CRISP-DM es sumamente flexible, y permite personalizar sus fases en función de los objetivos del proyecto.\\ 

Dentro del flujo de trabajo estructurado que propone la metodología CRISP-DM, se han definido dos procesos claves: en primer lugar, se lleva a cabo un proceso de Extracción, Transformación y Carga (ETL), orquestado por Apache Airflow, para consolidar los datos de consumo mensual de cada cliente en una curva representativa anual. Este proceso asegura la correcta integración, transformación y normalización de los datos para que las curvas representativas sean comparables entre sí.\\

Posteriormente, se desarrolla el proceso de agrupación, donde se determina el número óptimo de grupos de clientes a través de un análisis conjunto con las partes interesadas y el uso de métodos de validación como el método del codo. Se implementan y optimizan diferentes algoritmos de clustering, como KMeans, GaussianMixture, Birch y Spectral Clustering, para segmentar a los clientes en base a la similitud de sus curvas de consumo. Finalmente, se evalúan los resultados de cada algoritmo utilizando diversas métricas, como Silhouette Score, SSE, Davies-Bouldin Index y Calinski-Harabasz Index, para seleccionar el algoritmo que ofrezca las mejores agrupaciones. Los resultados obtenidos serán presentados tanto de manera visual como cuantitativa, permitiendo una interpretación clara y precisa de las agrupaciones logradas.

\section{Objetivo general}

Evaluar e implementar modelos de aprendizaje no supervisado para la segmentación de clientes no regulados del sector eléctrico utilizando curvas de carga para la obtención de agrupaciones homogéneas.

\section{Objetivos específicos}

\begin{enumerate}
    \item Levantar requerimientos para la obtención y procesamiento de los datos de consumo energético de los clientes no regulados, transformándolos en curvas de carga representativas para su almacenamiento en una base de datos.
    \item Realizar una revisión literaria de los algoritmos de agrupamientomásrelevantes, identificando su funcionamiento, principios y parámetros claves para su correcta optimización e implementación en la segmentación de clientes del sector eléctrico.
    \item Implementar una metodología de análisis de datos para la ejecución del proceso sistemático encargado de guiar las diferentes fases.
    \item Aplicar los algoritmos de clustering, utilizando métodos de validación para definir el número óptimo de agrupaciones.
    \item Evaluar y presentar los resultados generados por cada algoritmo, utilizando visualizaciones detalladas de las curvas de carga agrupadas.
\end{enumerate}

\section{Alcance}

Como se mencionó en la descripción del componente, el presente trabajo está enmarcado en el análisis y segmentación de clientes no regulados del sector eléctrico, a partir de la construcción de sus curvas de carga características y la posterior aplicación de algoritmos de aprendizaje no supervisado con el fin de identificar patrones de consumo energético. El alcance de este trabajo está definido bajo las siguientes consideraciones:

\begin{enumerate}
    \item Se ha adoptado la metodología CRISP-DM como marco de referencia, con una adaptación en su fase final. Dicha fase implica originalmente el despliegue del modelo en un entorno productivo, pero en este trabajo va a enfocarse en la evaluación comparativa de los resultados obtenidos con diferentes algoritmos de clustering, donde se presentarán métricas cuantitativas así como visualizaciones interpretativas de las agrupaciones.
    \item Se llevará a cabo un proceso ETL (Extracción, Transformación y Carga), el cual obtiene, integra, limpia y normaliza los registros históricos de consumo energético que se tienen de cada cliente, con la finalidad de generar curvas de carga que representen el comportamiento energético de cada cliente. Este proceso contempla la interpolación de valores nulos, la exclusión de días no laborales, corrección de formatos inconsistentes y la normalización mediante técnicas de escalamiento.
    \item Se realizará la optimización e implementación de varios algoritmos de clustering (KMeans, GaussianMixture, Birch y Spectral Clustering), estos fueron seleccionados en función de su relevancia en la literatura y su aplicabilidad en el análisis de análisis de curvas de carga. Para determinar el número óptimo de agrupaciones se hara uso de métodos de validación como el método del codo. Por otro lado, para la optimización de estos algoritmos se utilizará la correlación intra-cluster, esta métrica es la más adecuada pues captura de mejor manera la similitud en forma de las curvas agrupadas.
    \item Los resultados incluirán la curva de carga representativa de cada cliente, la curva de carga correspondiente al día de máxima demanda, archivos .csv con las coordenadas de dichas curvas. Asimismo, se presentarán resultados visuales de los clústeres y una tabla comparativa con métricas que cuantifican la calidad de las agrupaciones generadas por cada algoritmo.
    \item Para el desarrollo del presente componente se ha contemplado Python como lenguaje de programación de alto nivel, Visual Studio Code como entorno de desarrollo integrado, bibliotecas especializadas en análisis de datos y machine learning (pandas, scikit-learn, numpy, matplotlib, entre otras), así como herramientas de orquestación, en este caso Apache Airflow sobre Docker, para la automatización del proceso ETL.
\end{enumerate}

Por lo anterior expuesto el alcance del componente se limita a la construcción, aplicación y evaluación de modelos de clustering basados en la similitud de curvas de carga, sin abordar fases posteriores como despliegues productivos en entornos de la empresa distribuidora de energía.

% MARCO TEÓRICO

\section{Marco Teórico}

Para comprender este trabajo y su contexto, es de gran importancia tener bases sólidas sobre los principios subyacentes que sustentan el análisis y agrupación de los clientes en función de su curva de carga. Los apartados siguientes explicarán conceptos claves dentro del desarrollo del presente componente.

\subsection{Sobre el sector eléctrico}\\
\subsubsection{Clientes no regulados}\\
Los clientes no regulados en el sector eléctrico son aquellos cuya facturación por el suministro de energía se rige estrictamente por un contrato a término, el cual es realizado entre la empresa que suministra la energía y la empresa que recibe dicha energía. Los contratos mencionados anteriormente son bilaterales\cite{conelec2012}.\\

Debido a la naturaleza de los contratos que se suscriben con este tipo de clientes, los patrones de consumo de energía que poseen son bastantes variados respecto a los clientes regulados \cite{conelec2012}.


\subsubsection{Curvas típicas (curva de carga)}\\
Una curva de carga o también llamada curva típica es un registro gráfico que indica la demanda eléctrica que ha tenido un cliente en cada instante durante un intervalo de tiempo determinado\cite{curva_carga_1}.\\

Estas curvas de carga reflejan el patrón de consumo cotidiano que poseen los clientes, dicho patrón está directamente relacionado con las máquinas o aparatos que utilizan, así como la energía que consumen durante sus actividades\cite{curva_carga_2}.


\subsubsection{Importancia de segmentar a los clientes}\\

Debido a la naturaleza de los clientes no regulados y, agregando el hecho de que en su mayoría son grandes clientes, segmentarlos en grupos homogéneos permite optimizar la gestión de la demanda y mejorar la planificación del suministro eléctrico. Al agrupar clientes con patrones de consumo similares, es posible diseñar estrategias más eficientes para la contratación de energía, desarrollar y optimizar modelos tarifarios y, mejorar la predicción de la demanda a futuro \cite{curva_carga_1}. Además, esta segmentación ayuda a evitar el sobredimensionamiento o subdimensionamiento de la capacidad de generación y distribución, garantizando un uso más eficiente de los recursos y optimizando los costos operativos.

\subsection{Metodología CRISP-DM}\\
CRISP-DM, cuyas siglas corresponden a Cross-Industry Standard Process for Data Mining, es un método probado utilizado para orientar proyectos de minería de datos. Ofrece una serie de fases que resúmen el ciclo vital de minería de datos, a la vez que incluye descripciones y tareas necesarias en cada fase, ayudando a estructurar un flujo de trabajo ordenado cuya secuencia no es estricta, donde se puede avanzar y retroceder entre fases de ser necesario \cite{crisp-dm1}.\\

El modelo CRISP-DM es sumamente flexible, y sus fases pueden ser personalizadas en función de los objetivos del proyecto, pudiendo crear un modelo de minería de datos que se adapte a necesidades concretas \cite{crisp-dm1}. CRISP-DM contiene un total de seis fases, tal y como se describe en \cite{crisp-dm2}:

\begin{enumerate}
    \item \textbf{Comprensión del negocio:} Esta fase inicial se enfoca en analizar y comprender tanto los objetivos como los requerimientos del proyecto desde la perspectiva del negocio. Posteriormente todo este conocimiento es plasmado en un proyecto de minería de datos enfocado en alcanzar los objetivos.
    
    \item \textbf{Comprensión de los datos:} La fase de comprensión de datos tiene como principal objetivo la 'familiarización' con los datos. Para lograr esto se realiza una recolección inicial de los datos y se procede a realizar un pequeño análisis exploratorio de los datos con el fin de comprender los datos que se tienen e identificar problemas con la calidad de los mismos.
    
    \item \textbf{Preparación de los datos:} Esta fase es crucial en CRISP-DM, debido a que abarca todas las actividades requeridas hasta la construcción final del conjunto de datos, los cuales servirán posteriormente para la fase de modelado. Esta fase incluye tareas como la limpieza, transformación y normalización de los datos, con el fin de asegurar la calidad de estos.
    
    \item \textbf{Modelado:} Varias herramientas de modelamiento son seleccionadas con el fin de ser aplicadas sobre nuestro conjunto de datos preparados. Los parámetros de dichas herramientas deben ser calibrados hasta obtener los valores óptimos que ofrezcan los mejores resultados.
    
    \item \textbf{Evaluación:} En esta penúltima fase del proyecto, ya se tiene construido uno o varios modelos que aparentemente ofrecen resultados de calidad. Antes de proceder a la fase del despliegue, se realiza una evaluación del modelo, revisando cada paso ejecutado hasta la construcción final del mismo con el fin de determinar si existe algún objetivo que no haya sido abordado lo suficiente.
    
    \item \textbf{Despliegue:} La construcción del modelo no es el final del proyecto. En función de los requerimientos, la fase de despliegue puede ser tan simple como la generación de un reporte o tan complejo como su respectiva implementación en otros proyectos de minería de datos.
    
\end{enumerate}


\subsection{Minería de datos}
Según \cite{datamining-1}, la minería de datos corresponde a un proceso que consiste en la extracción de información relevante a partir de un gran conjunto de datos, con el fin de encontrar patrones interesantes que sean de utilidad, los cuales de otro modo habrían pasado desapercibidos. De la misma manera, métodos tradicionales de análisis de datos son combinados con algoritmos capaces de manejar grandes volúmenes de datos \cite{datamining-2}.\\

Entre sus principales funciones se destacan \cite{datamining-2}:
\begin{enumerate}
\item \textbf{Caracterización/Discriminación:} Sintetizar y explicar clases o conceptos.
\item \textbf{Patrones frecuentes y asociaciones:} Reconocer relaciones que se repiten en el conjunto de datos.
\item \textbf{Clasificación y regresión:} Elaborar modelos para predecir clases o valores numéricos.
\item \textbf{Agrupación:} Generar etiquetas a partir de datos sin clasificar, optimizando la similitud interior.
\item \textbf{Detección de valores atípicos:} Reconocer datos que no se ajustan a un patrón general.
\end{enumerate}

\subsection{Proceso ETL}
El proceso ETL (Extracción, Transformación y Carga), es una técnica crucial que sirve para obtener, organizar y usar los datos apropiadamente según el fin requerido, se enfoca principalmente en la unión de datos provenientes de diversas fuentes, así como de su evaluación y limpieza \cite{singu2022etl}. Tal y como sus siglas indican, este proceso involucra tres fases:

\begin{enumerate}
    \item \textbf{Extracción}: Este paso es el responsable de extraer el conjunto requerido de datos de una o más fuentes, donde cada fuente  tiene sus propias características, por lo cual, se debe tener conocimiento sobre como acceder a dichas fuentes, comprender la estructura de las mismas y saber como manejar cada fuente de acuerdo a su naturaleza \cite{elsappagh2011proposed}. Este proceso termina cuando todo el conjunto de datos es consolidado en un solo repositorio \cite{elsappagh2011proposed}.
    \item \textbf{Transformación}: Esta segunda fase consiste en procesar los datos extraídos para que sean consistentes, limpios e integrables dentro del repositorio. Se realizan diversas tareas como reestructurar la información, convertir formatos, limpiar los datos, integrar múltiples fuentes, tratamiento de valores nulos, entre otros \cite{inmon2002building}. El objetivo es asegurar que la información esté depurada y en condiciones para su carga en el repositorio final \cite{inmon2002building}.
    \item \textbf{Carga}: Es la última fase, aquí los datos son almacenados en un repositorio final o en una base de datos para su posterior análisis \cite{gour2010improve}.
\end{enumerate}

\subsection{Aprendizaje no supervisado}
El aprendizaje no supervisado es un tipo de algoritmo de aprendizaje automático, utiliza únicamente datos sin etiquetar, y es usado sobre estos con el objetivo de descubrir patrones o agrupar datos que posiblemente comparten características similares entre sí \cite{Wang2022_unsupervised_machine_learning_urban_studies}.

\subsubsection{Clustering}
Es una de las categorías del aprendizaje no supervisado, la más consolidada en la actualidad, su objetivo es la identificación de subgrupos dentro de un conjunto extenso de datos no procesados, estos subgrupos son encontrados mediante la diferenciación de características \cite{Wang2022_unsupervised_machine_learning_urban_studies}.
\subsubsection{Número de agrupaciones}
Un problema muy común al utilizar algoritmos de aprendizaje no supervisado es elegir el número de agrupaciones deseadas \cite{CoraggioCoretto2021_clusters_quadratic_discriminant_score}, esta elección es muy importante debido a que puede alterar la calidad de las agrupaciones finales dadas por los algoritmos. Como se menciona en \cite{Lezhnina2022_latent_class_cluster_analysis}, esta elección puede ser totalmente subjetiva, y en la mayoría de los casos el números de agrupaciones es seleccionado en función de criterios preestablecidos, sin embargo, existen técnicas como el método del codo que ayudan a validar el número de agrupaciones y que pueden ayudar en la selección de este critero.
\subsubsection{Metodo del codo}
Es la forma más habitual de elegir o validar el número de clústeres, este método consiste en ajustar varios modelos K-means para un rango específico de agrupaciones, normalmente desde 1 hasta un número arbitrario máximo, posteriormente se traza un gráfico que contiene el valor total de la suma de los cuadrados por cada número de clústeres frente a ese respectivo número de clústeres \cite{Friedman2017_survey_R_packages_cluster_analysis}. El objetivo es encontrar aquel valor de número de clústeres donde la gráfica muestra un 'codo' y elegir dicho valor que probablemente nos ofrezca grupos bien separados \cite{Friedman2017_survey_R_packages_cluster_analysis}.

\subsubsection{Algoritmos de clustering}
Los algoritmos de clustering son una parte fundamental del aprendizaje no supervisado, pues facilitan el descubrimiento de estructuras y patrones ocultos dentro de un conjunto de datos sin etiquetar \cite{wani2024comprehensive}.\\

A continuación se describirán los algoritmos de clustering que van a ser utilizados para el desarrollo del presente componente: 

\begin{enumerate}
    \item \textbf{K-Means} \\
    Algoritmo de clustering basado en centroides que organiza $n$ puntos de datos en $k$ clústeres según la proximidad a centroides representativos \cite{wani2024comprehensive}. 
    Cada centroide corresponde a la media de su clúster y el objetivo es minimizar la suma de las distancias al cuadrado entre cada punto y su centroide \cite{wani2024comprehensive}, se puede formular matemáticamente como:
    
    {\setlength{\abovedisplayskip}{-8pt}
    \setlength{\belowdisplayskip}{8pt}\begin{equation}
        J = \sum_{i=1}^{k} \sum_{\mathbf{x}\in S_i} \|\mathbf{x} - \mu_i\|^2 
        \quad \text{con} \quad 
        \mu_i = \frac{1}{|S_i|}\sum_{\mathbf{x}\in S_i} \mathbf{x} 
        \quad \text{y} \quad 
        i = \arg\min_j \|\mathbf{x} - \mu_j\|^2
    \end{equation}}

    \item \textbf{Gaussian Mixture Models (GMM)} \\
    Modelo que asume que los datos provienen de una mezcla de Gaussianas, cada una definida por su media y covarianza \cite{wani2024comprehensive}. 
    Este enfoque permite representar estructuras multimodales donde K-means falla. 
    Los parámetros se estiman con el algoritmo EM, que ajusta iterativamente medias, covarianzas y pesos para representar mejor los datos \cite{wani2024comprehensive}. Matemáticamente, el modelo es expresado como:
    
    {\setlength{\abovedisplayskip}{-8pt}
    \setlength{\belowdisplayskip}{8pt}\begin{equation}
        p(x) = \sum_{j=1}^{k} \pi_j N(x|\mu_j, \Sigma_j), 
        \qquad 
        w_{ij} = \frac{\pi_j N(x_i|\mu_j, \Sigma_j)}
        {\sum_{l=1}^{k} \pi_l N(x_i|\mu_l, \Sigma_l)}
    \end{equation}}
    
    mientras que las actualizaciones de los parámetros en cada iteración están dadas por:

    {\setlength{\abovedisplayskip}{-2pt}
    \setlength{\belowdisplayskip}{8pt}\begin{equation}
        \pi_j = \frac{1}{n}\sum_{i=1}^{n} w_{ij}, 
        \quad 
        \mu_j = \frac{\sum_{i=1}^{n} w_{ij} x_i}{\sum_{i=1}^{n} w_{ij}}, 
        \quad 
        \Sigma_j = \frac{\sum_{i=1}^{n} w_{ij}(x_i-\mu_j)(x_i-\mu_j)^{T}}
        {\sum_{i=1}^{n} w_{ij}}
    \end{equation}}
        

    \item \textbf{Spectral Clustering} \\
    Algoritmo de clustering basado en grafos, transforma los datos en una red donde los nodos representan puntos de datos y las aristas sus similitudes, a partir de esto construye la matriz Laplaciana, cuyos autovectores permiten identificar estructuras dentro del grafo y formar clústeres con alta cohesión interna \cite{wani2024comprehensive}. El objetivo es minimizar la siguiente función:
    
    {\setlength{\abovedisplayskip}{-20pt}
    \setlength{\belowdisplayskip}{8pt}\begin{equation}
        \min \; \text{Tr}(H^{T} L H) \quad \text{sujeto a} \quad H^{T}H = I
    \end{equation}}
        

    \item \textbf{BIRCH} \\
    Es un algoritmo de clustering de tipo jerárquico, está diseñado para trabajar con grandes volúmenes de datos, resumiendo toda la información de los mismos en una sola estructura jerárquica que tiene el nombre de CF-Tree, donde cada clúster es representado como una Clustering Feature (CF) \cite{fontanini2018birch}, la cual está definida por:

    {\setlength{\abovedisplayskip}{1pt}
    \setlength{\belowdisplayskip}{8pt}\begin{equation}
    CF = (N, LS, SS)
    \end{equation}}
    
    donde $N$ es el número de puntos, $LS$ la suma lineal y $SS$ la suma de los cuadrados de los datos. El umbral de radio $T$ se determina mediante un problema de optimización, definido como:

    {\setlength{\abovedisplayskip}{1pt}
    \setlength{\belowdisplayskip}{3pt}\begin{equation}
    \min_T \; g(W_k(T), B_k(T))
    \end{equation}}
    
    donde $W_k$ mide la compacidad intra-clúster y $B_k$ la separación inter-clúster \cite{fontanini2018birch}.

\end{enumerate}



\subsubsection{Hiperparametrización de algoritmos}
Es una técnica que consiste en ajustar los parámetros que controlan el comportamiento de los algoritmos de clustering, estos parámetros influyen directamente en la calidad de las agrupaciones finales, el objetivo es encontrar aquella combinación de parámetros que ofrezca los mejores resultados en cada algoritmo \cite{pathak2024randomized}. 

\subsection{Métricas de evaluación de agrupaciones}
Son medidas de calidad que sirven para dar validación a los clústeres obtenidos por los algoritmos, estas métricas se basan en la premisa de 'Maximizar la similitud dentro de cada clúster y minimizar la similitud entre los diferentes clústeres', el objetivo es lograr clústeres compactos y lo más separados posibles entre sí \cite{heras2023tfg}.

\vspace{-0.025cm}

\subsection{Herramientas utilizadas}
Para el desarrollo del componente se han considerado varias herramientas que facilitan las etapas de procesamiento, almacenamiento, análisis de los datos, e implementación de los modelos de clustering, la Tabla \ref{tab:herramientas} los detalla:

\vspace{-0.025cm}

%\begin{footnotesize}
\begin{longtable}{|p{2.5cm}|p{12.3cm}|}
\caption{{{Herramientas utilizadas para el desarrollo del componente}} \label{tab:herramientas}} \\
\hline
\textbf{Herramienta} & \textbf{Descripción de la herramienta} \\
\hline
\endfirsthead

\hline
\textbf{Herramienta} & \textbf{Descripción y función en la tesis} \\
\hline
\endhead

\hline
\endfoot

\hline
\endlastfoot

Airflow & Apache Airflow es una plataforma de código abierto que permite el desarrollo, programación y supervisión flujos de trabajo, utiliza Python, lo que le permite conectarse con diversas tecnologías \cite{airflow_docs}. \\ \hline

Docker & Docker es una plataforma abierta utilizada para el desarrollo, envío y ejecución de aplicaciones, permite empaquetar y ejecutar aplicaciones en un entorno aislado denominado contenedor \cite{docker_docs}. \\ \hline

Python & Python es un lenguaje de programación de alto nivel con naturaleza interpretada, maneja estructuras de datos con un alto nivel de eficiencia y ofrece una sintaxis simple, razones por las cuales es ampliamente utilizado en campos como desarrollo web, ciencia de datos, automatización, entre otros \cite{python_docs}. \\ \hline

Visual Studio Code & Visual Studio Code es un editor de código fuente que contiene herramientas de depuración, control de versiones y extensiones para varios lenguajes. Ofrece varias características que permiten desarrollar código eficientemente \cite{vscode_docs}.\\ \hline

MongoDB & MongoDB es una base de datos no relacional basada en documentos, ofrece una gran escalabilidad y flexibilidad, además de un modelo avanzado de consultas e indexación \cite{mongodb_docs}.\\
\hline

\end{longtable}
%\end{footnotesize}

% METODOLOGÍA






\chapter{Metodología}
Describir el diseño o el planteamiento utilizado...

\subsection{Flujo de trabajo propuesto}

\begin{itemize}
    \item \textbf{Extracción, Transformación y Carga de los Datos (ETL):}  
    La primera fase consiste en la extracción, transformación y carga (ETL) de los datos de consumo energético. Los datos iniciales provienen de archivos de consumo mensual por cliente. En este paso, se construye un archivo anual para cada cliente, en el cual se agregan y consolidan los datos correspondientes a cada año. Además, los datos son escalados y normalizados para garantizar su consistencia y comparabilidad. Este proceso se lleva a cabo con la ayuda de \textbf{Apache Airflow}, el cual permite automatizar el flujo de trabajo y garantizar su ejecución eficiente. Finalmente, los datos transformados son cargados en \textbf{MongoDB}, asegurando su disponibilidad para las fases siguientes del análisis.

    \item \textbf{Segmentación de Clientes:}  
    En esta fase, se procede a definir el número óptimo de grupos de clientes a través de un análisis conversacional con las partes interesadas y la aplicación de métodos como el \textbf{método del codo}. Una vez definido el número de grupos, se seleccionan y optimizan los algoritmos de agrupación más adecuados para el análisis, tales como \textbf{KMeans}, \textbf{GaussianMixture}, \textbf{Birch} y \textbf{Spectral Clustering}. Estos algoritmos se utilizan para agrupar a los clientes según la similitud de sus curvas de consumo energético anual. Los resultados obtenidos se presentan visualmente, permitiendo observar las agrupaciones y patrones emergentes en el consumo de energía de los clientes.

    \item \textbf{Evaluación Comparativa de los Algoritmos:}  
    Finalmente, se realiza una evaluación comparativa de los algoritmos de agrupación aplicados, utilizando diversas métricas para medir la calidad de las agrupaciones. Entre las métricas utilizadas se encuentran el \textbf{Silhouette Score}, \textbf{SSE (Suma de Errores al Cuadrado)}, \textbf{DBI (Índice de la Diferencia de Davies-Bouldin)} y \textbf{CHI (Índice de Calinski-Harabasz)}. Estas métricas permiten analizar el rendimiento de los algoritmos y seleccionar el que mejor se adapte a los datos de consumo energético de los clientes.
\end{itemize}


\begin{enumerate}
     

\item \textbf{Proceso ETL}\\
El primer proceso consiste en el desarrollo de un flujo ETL bajo el marco de trabajo de Apache Airflow, este proceso nos permitirá estructurar y preparar los datos de consumo de los clientes en curvas anuales características para su posterior análisis de agrupación. La \autoref{fig:fig1} ilustra de manera detallada todas las etapas que abarca este proceso.\\

\renewcommand{\thefigure}{\arabic{figure}}

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.7]{./imgs/ETL_process.png}
\caption{{Proceso ETL con sus etapas}}
\label{fig:fig1}
\end{center}
\end{figure}

\item \textbf{Proceso de agrupación}\\
El segundo proceso comprende todo el proceso de agrupamiento, en este apartado se elegirá el número de agrupaciones deseadas, se seleccionarán y aplicarán diversos algoritmos de agrupamiento para finalmente evaluar la calidad de las agrupaciones obtenidas por cada algoritmo. La \autoref{fig:fig2} ilustra de manera detallada todas las etapas que abarca este proceso.\\

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.7]{./imgs/Clustering_process.png}
\caption{{Proceso de agrupación con sus etapas}}
\label{fig:fig2}
\end{center}
\end{figure}

\end{enumerate}

Los datos utilizados para el presente componente comprenden todas las mediciones mensuales del año 2024 por cada cliente, las cuales han sido obtenidos de la página de telemediciones de la Empresa Eléctrica de Quito.\\
    
Por otro lado, la segmentación de clientes se realiza exclusivamente en función de la forma de su curva característica anual, obtenida al final del proceso ETL descrito en la \autoref{fig:fig1}. No se consideran otros factores, como las tarifas o la geolocalización, ya que el objetivo de la parte interesada es agrupar a los clientes estrictamente según el patrón de consumo de energía reflejado en su curva característica anual.\\

% RESULTADOS, CONCLUSIONES Y RECOMENDACIONES


\chapter{Resultados, Conclusiones y Recomendaciones}
\section{Resultados}
Ejemplo de tabla:
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        No. Prueba & Resultado & Tiempo [s] \\
        \hline
        1 & 10 & 0.9 \\
        2 & 5 & 0.5 \\
        \hline
    \end{tabular}
    \caption{Resultados de las pruebas realizadas}
\end{table}

\section{Conclusiones}
\section{Recomendaciones}


% REFERENCIAS BIBLIOGRÁFICAS
\chapter{Referencias Bibliográficas}

\printbibliography[heading=none]

Ejemplo IEEE:
\begin{itemize}
    \item \textbf{[1]} L. Carvajal, \textit{Metodología de la Investigación Científica}. Santiago de Cali: U.S.C., 2006.
\end{itemize}



https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ccs2.12080\\

https://hpi.de/fileadmin/user_upload/fachgebiete/naumann/publications/2008/ETL_Management.pdf\\

https://www.researchgate.net/profile/Sameer_Shukla3/publication/369899578_Developing_Pragmatic_Data_Pipelines_using_Apache_Airflow_on_Google_Cloud_Platform/links/647953c7b3dfd73b7759022a/Developing-Pragmatic-Data-Pipelines-using-Apache-Airflow-on-Google-Cloud-Platform.pdf?origin=journalDetail&_tp=eyJwYWdlIjoiam91cm5hbERldGFpbCJ9\\

https://www.controlrecursosyenergia.gob.ec/wp-content/uploads/downloads/2021/03/Folleto-Resumen-Estad%C3%ADsticas-2011.pdf\\



\chapter{Anexos}
\section*{Anexo I. Conjunto de Datos Extensos}
\section*{Anexo II. Formato de Entrevista}
\section*{Anexo III. Enlaces}

\end{document}