\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{setspace}
\usepackage{caption}
\captionsetup[table]{width=\textwidth}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{titletoc}
\usepackage{amsfonts}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage[spanish]{babel}
\addto\captionsspanish{\renewcommand{\figurename}{Figura}}
\usepackage{hyperref}
\hypersetup{
    pdfborder={0 0 0}
}
\usepackage[spanish]{babel}
\def\figureautorefname{Figura}
\setcounter{secnumdepth}{3}
\usepackage[style=ieee, backend=biber]{biblatex}
\addbibresource{referencias.bib}
\DefineBibliographyStrings{spanish}{
    url = {Obtenido de:},
    urlseen = {Consultado el},
}
\usepackage{url}
\usepackage{xcolor}
\newcommand{\headlinecolor}{\color{black}}


% Márgenes y espaciado
\geometry{left=3cm, right=2.5cm, top=3cm, bottom=2.5cm}
\newcommand{\hsp}{\hspace{-5pt}}
% Número a la derecha de cada capítulo y espaciado corregido
\titleformat{\chapter}[hang]
  {\vspace{-1cm}
  \headlinecolor\Large\bfseries} % formato
  {\thechapter.\hsp}             % número
  {10pt}                         % separación número - título
  {\Large\bfseries}               % formato del título

% Espaciados capítulos: {izquierda}{antes}{después}
\titlespacing*{\chapter}{0pt}{-8pt}{16pt}

% Espaciados secciones
\titlespacing*{\section}{0pt}{14pt}{8pt}
\titlespacing*{\subsection}{0pt}{10pt}{6pt}

% Para que todo el documento tenga el tipo de letra Arial
\usepackage{fontspec}

% Selecciona Arial como fuente principal
\setmainfont{Arial}

\begin{document}

\renewcommand{\tablename}{Tabla}
\renewcommand{\figurename}{Figura}

\pagenumbering{gobble} % ELIMINA NUMERACION EN PORTADA
\begin{titlepage}
\renewcommand{\baselinestretch}{1.4}\normalsize

\newgeometry{left=3cm,right=3cm,top=3cm,bottom=3cm}
\centering

{\bfseries\fontsize{24}{24}\selectfont ESCUELA POLITÉCNICA NACIONAL\par}\vspace{1.5cm}

{\bfseries\fontsize{16}{20}\selectfont FACULTAD DE INGENIERÍA EN SISTEMAS\par}\vspace{1.5cm}

{\bfseries\fontsize{14}{17}\selectfont OPTIMIZACIÓN DE SISTEMAS DE INFORMACIÓN EN CONTEXTOS EMPRESARIALES\par}\vspace{1cm}

{\bfseries\fontsize{14}{17}\selectfont ANÁLISIS Y SEGMENTACIÓN DE CLIENTES NO REGULADOS DEL SECTOR ELÉCTRICO MEDIANTE ALGORITMOS DE APRENDIZAJE NO SUPERVISADO\par}\vspace{1.5cm}

{\bfseries\fontsize{12}{14}\selectfont TRABAJO DE INTEGRACIÓN CURRICULAR PRESENTADO COMO REQUISITO PARA LA OBTENCIÓN DEL TÍTULO DE INGENIERO EN CIENCIAS DE LA COMPUTACIÓN\par}\vspace{1.5cm}

{\bfseries\fontsize{12}{14}\selectfont ANDRÉS ANTONIO ZAMBRANO ALQUINGA\par}
{\fontsize{12}{14}\selectfont andres.zambrano03@epn.edu.ec\par}\vspace{1cm}

{\bfseries\fontsize{12}{14}\selectfont DIRECTOR: JOSAFÁ DE JESÚS AGUIAR PONTES\par}
{\fontsize{12}{14}\selectfont josafa.aguiar@epn.edu.ec\par}\vspace{1.5cm}

{\bfseries\fontsize{12}{14}\selectfont DMQ, julio 2025\par}
    
\restoregeometry
\end{titlepage}


%% CERTIFICACIONES

\renewcommand{\baselinestretch}{1.25}\normalsize

\pagenumbering{roman} % NUMERACION EN ROMANO
\chapter*{Certificaciones}
\addcontentsline{toc}{chapter}{Certificaciones}

Yo, \textbf{Andrés Zambrano}, declaro que el trabajo de integración curricular aquí descrito es de mi autoría; que no ha sido previamente presentado para ningún grado o calificación profesional; y, que he consultado las referencias bibliográficas que se incluyen en este documento.\\[2cm]
\textbf{NOMBRE\_ESTUDIANTE}\\[1cm]

Certifico que el presente trabajo de integración curricular fue desarrollado por Andrés Zambrano, bajo mi supervisión.\\[2cm]

\textbf{NOMBRE\_DIRECTOR}\\ \textbf{DIRECTOR}



%% DECLARATORIA DE AUTORIA



\chapter*{Declaración de autoría}
\addcontentsline{toc}{chapter}{Declaración de autoría}

A través de la presente declaración, afirmamos que el trabajo de integración curricular aquí descrito, así como el (los) producto(s) resultante(s) del mismo, son públicos y estarán a disposición de la comunidad a través del repositorio institucional de la Escuela Politécnica Nacional; sin embargo, la titularidad de los derechos patrimoniales nos corresponde a los autores que hemos contribuido en el desarrollo del presente trabajo; observando para el efecto las disposiciones establecidas por el órgano competente en propiedad intelectual, la normativa interna y demás normas.\\[2cm]
\textbf{Andrés Zambrano}\\ \textbf{Josafá Aguiar}


% DEDICATORIA


\chapter*{Dedicatoria}
\addcontentsline{toc}{chapter}{Dedicatoria}

A mis padres celestiales, Dios y la santísima Virgen María, en quienes siempre he depositado toda mi fé y confianza a lo largo de toda mi trayectoria académica.\\

\vspace{-2mm}

A mis padres, Verito y Marco, quienes a pesar de todas las dificultades que se presentaron a lo largo del camino, nunca dudaron de mí, y en su lugar, siempre supieron alentarme y darme su apoyo incondicional para seguir adelante, sin lugar a dudas, este, y todos mis logros se los dedico a ustedes. \\

\vspace{-2mm}

A Edita Vélez, mi segunda mamá, quien me cuidó durante toda mi niñez, llenándome siempre de amor, mimos y mucho cariño.\\

\vspace{-2mm}

A mis padrinos, Franklin Vásquez y Silvana Barba, por acogerme con cariño en su hogar durante mis estudios universitarios, de igual manera, a mis primos, Carolina, Dennis y Pamela, quienes más que primos han sido como hermanos para mí.\\

\vspace{-2mm}

A mi primo Jhonny Sánchez, quien ha sido como un hermano para mí, con quien he compartido invaluables momentos durante gran parte de mi niñez. Gracias por ser ese hermano que nunca pude tener, pero que la vida se encargó de darme.\\

\vspace{-2mm}

A la memoria de mis abuelitos, Teresa y Manuel, quienes a pesar de ya no estar físicamente conmigo, sigo sintiendo su amor y protección en cada paso que doy.\\

\vspace{-2mm}

A mis amigos, compañeros de risas, retos e innumerables experiencias, que siempre han estado presentes, tanto en las buenas como en las malas. \\

\vspace{-2mm}

A toda mi familia en general, quienes de manera directa o indirecta han contribuido con su granito de arena para formar la persona que soy hoy en día.\\

\vspace{-2mm}

Finalmente, a mis dos peluditos, Rockie y Merlín, especialmente a mi gordo, Merlín, mi más linda compañía durante mi transición por propedéutico, pasó largas noches de vela a mi lado brindándome de su cálida compañía mientras yo estudiaba.




% AGRADECIMIENTOS




\chapter*{Agradecimientos}
\addcontentsline{toc}{chapter}{Agradecimientos}
Agradezco en primer lugar, a Dios y a la Vírgen María por no desampararme nunca en ninguna etapa de mi vida, por haberme guiado en cada momento, y por empaparme de sabiduría durante toda mi transición por la universidad.\\

\vspace{-2mm}

A mis padres, mis dos grandes tesoros, gracias por creer en mí en todo momento, por demostrarme que con esfuerzo y dedicación todo es posible y, sobre todo, por su amor y apoyo incondicional. Gracias por tanto, gracias por ser mis padres.\\

\vspace{-2mm}

Quiero agradecer de manera muy especial a mi prima Carolina Vásquez por todo lo que ha hecho por mí. Gracias Carito por ser una guía indispensable y un apoyo incondicional en mi vida, eres como una hermana para mí.\\

\vspace{-2mm}

Quiero expresar mi más profundo agradecimiento al Ing. Boris Astudillo por su invaluable orientación, sus sabios consejos y por su constante guía y apoyo a lo largo de mi formación universitaria, en particular durante el desarrollo de mi proyecto de titulación.\\

\vspace{-2mm}

A mi alma máter, la Escuela Politécnica Nacional y a los docentes que contribuyeron a mi formación académica, por brindarme todos los conocimientos y las herramientas necesarias para desarrollarme como profesional.\\

\vspace{-2mm}

Quiero agradecer a todo el equipo de la Empresa Eléctrica Quito, por su apoyo y guía durante el desarrollo de mis prácticas preprofesionales, en especial a los ingenieros e ingenieras Carolina, William, Oscar, Claudia, Isabel y Grace. Agradezco de igual manera al ingeniero Ricardo Dávila por brindarme la confianza y la oportunidad de vivir esta experiencia invaluable para mi desarrollo profesional.\\

\vspace{-2mm}

Finalmente, agradezco a mis amigos Carlos, Alexis, Hernán, Galo, Dilan y los que faltan por nombrar, por hacer que la vida universitaria fuera mucho más llevadera. Gracias por todas las experiencias que compartimos, risas, enojos, tristezas, largas charlas, y sobre todo, la remontada del siglo en sexto semestre.


% ÍNDICE DE CONTENIDOS


\tableofcontents


% RESUMEN


\chapter*{Resumen}

Este Trabajo de Integración Curricular aborda un proyecto de minería de datos enfocado en la implementación de un algoritmo de aprendizaje no supervisado para segmentar clientes en grupos homogéneos a partir de sus curvas características de consumo anual. El objetivo es identificar patrones de consumo energético que permitan una planificación más eficiente y una optimización del uso de la energía en el sector eléctrico.\\

La metodología aplicada es CRISP-DM, con una modificación en su fase final. Dentro de la misma, se han planteado dos procesos claves a seguir: en primer lugar, se desarrolla un proceso ETL orquestado por Apache Airflow, para la consolidación y transformación de los datos mensuales en una curva característica representativa anual por cada cliente, posteriormente, en el proceso de agrupación, se seleccionan y optimizan varios algoritmos para agrupar a los clientes en base a la similitud de sus curvas de consumo.\\

Los resultados de cada algoritmo son evaluados mediante diversas métricas, que cuantifican la calidad de las agrupaciones, con el fin de determinar el algoritmo que ofrece las agrupaciones de mejor calidad. Los resultados de agrupación serán presentados de manera visual y cuantitativa.\\


\textbf{Palabras clave:} minería de datos, segmentación de clientes, curvas de consumo, aprendizaje no supervisado, algoritmos de clustering, planificación energética, proceso ETL, Apache Airflow, CRISP-DM.


% ABSTRACT


\chapter*{Abstract}

This Curriculum Integration Project focuses on a data mining project aimed at implementing an unsupervised learning algorithm to segment clients into homogeneous groups based on their annual characteristic consumption curves. The goal is to identify energy consumption patterns that allow for more efficient planning and optimization of energy use in the electric sector.\\

The methodology applied is CRISP-DM, with a modification in its final phase. Within this framework, two key processes are followed: first, an ETL process orchestrated by Apache Airflow is developed to consolidate and transform monthly data into an annual representative characteristic curve for each client. Then, in the grouping process, several algorithms are selected and optimized to group clients based on the similarity of their consumption curves.\\

The results of each algorithm are evaluated using various metrics that quantify the quality of the groupings, in order to determine which algorithm provides the highest-quality groupings. The grouping results will be presented both visually and quantitatively.\\

\textbf{Keywords:} data mining, customer segmentation, consumption curves, unsupervised learning, clustering algorithms, energy planning, ETL process, Apache Airflow, CRISP-DM.

\newpage
\pagenumbering{arabic} % NUMERACION NORMAL

% INTRODUCCIÓN

\chapter{DESCRIPCIÓN DEL COMPONENTE DESARROLLADO}


En el contexto actual de las empresas proveedoras de energía, como la Empresa Eléctrica Quito (EEQ), la eficiente gestión energética es uno de los principales desafíos a enfrentar. Múltiples factores como la diversificación en los hábitos de consumo y variablidad de la demanda dificultan la planificación y diseño de estrategias eficientes que permitan responder de manera adecuada. Los métodos tradicionales de análisis, que se basan en promedios o clasificaciones rígidas resultan insuficientes para capturar dicha complejidad en los patrones de consumo de los clientes, dificultando el diseño de una planificación energética eficiente.\\

Con el fin de optimizar la distribución de recursos en áreas como la gestión tarifaria y la distribución eléctrica, es fundamental analizar los patrones de consumo. La identificación de estos patrones en el comportamiento energético de los clientes brinda la posibilidad de definir segmentos con características similares, permitiendo a las compañías proveedoras de energía establecer una base más firme para la toma de decisiones.\\

Ante esta problemática, se ha desarrollado un componente orientado a la segmentación inteligente de clientes, implementando un proyecto de minería de datos que propone un enfoque basado en técnicas de aprendizaje no supervisado con el fin de generar grupos homogéneos en función de la forma de su curva característica anual de consumo energético. El objetivo principal es identificar patrones de consumo que permitan una planificación más eficiente y optimización del uso de la energía en el sector eléctrico.\\

Bajo este contexto, el desarrollo del componente es realizado bajo la metodología CRISP-DM, con una ligera modificación en su fase final. Mientras que en la metodología original la fase final se centra en la implementación y despliegue del modelo, en este caso, el objetivo final es, entre todas las agrupaciones dadas por los diferentes algoritmos, escoger aquella que tenga la mejor calidad y homogeneidad, basándose en métricas de evaluación. Esta modificación de la fase final es posible debido a que CRISP-DM es sumamente flexible, y permite personalizar sus fases en función de los objetivos del proyecto.\\ 

Dentro del flujo de trabajo estructurado que propone la metodología CRISP-DM, se han definido dos procesos claves: en primer lugar, se lleva a cabo un proceso de Extracción, Transformación y Carga (ETL), orquestado por Apache Airflow, para normalizar y consolidar los datos de consumo mensual de cada cliente en una curva representativa anual.\\

Posteriormente, se desarrolla el proceso de agrupación, donde se determina el número óptimo de grupos de clientes a través de un análisis conjunto con las partes interesadas y el uso de métodos de validación como el método del codo. Se implementan y optimizan diferentes algoritmos de clustering, como KMeans, GaussianMixture, Birch y Spectral Clustering, para segmentar a los clientes en base a la similitud de sus curvas de consumo. Finalmente, se evalúan los resultados de cada algoritmo utilizando diversas métricas, como Silhouette Score, SSE, Davies-Bouldin Index y Calinski-Harabasz Index, para seleccionar el algoritmo que ofrezca las mejores agrupaciones. Los resultados obtenidos serán presentados tanto de manera visual como cuantitativa, permitiendo una interpretación clara y precisa de las agrupaciones logradas.

\section{Objetivo general}

Evaluar e implementar modelos de aprendizaje no supervisado para la segmentación de clientes no regulados del sector eléctrico utilizando curvas de carga para la obtención de agrupaciones homogéneas.

\section{Objetivos específicos}

\begin{enumerate}
    \item Levantar requerimientos para la obtención y procesamiento de los datos de consumo energético de los clientes no regulados, transformándolos en curvas de carga representativas para su almacenamiento en una base de datos.
    \item Realizar una revisión literaria de los algoritmos de agrupamiento más relevantes, identificando su funcionamiento, principios y parámetros claves para su correcta optimización e implementación en la segmentación de clientes del sector eléctrico.
    \item Implementar una metodología de análisis de datos para la ejecución del proceso sistemático encargado de guiar las diferentes fases.
    \item Aplicar los algoritmos de clustering, utilizando métodos de validación para definir el número óptimo de agrupaciones.
    \item Evaluar y presentar los resultados generados por cada algoritmo, utilizando visualizaciones detalladas de las curvas de carga agrupadas.
\end{enumerate}

\section{Alcance}

Como se mencionó en la descripción del componente, el presente trabajo está enmarcado en el análisis y segmentación de clientes no regulados del sector eléctrico, a partir de la construcción de sus curvas de carga características y la posterior aplicación de algoritmos de aprendizaje no supervisado con el fin de identificar patrones de consumo energético. El alcance de este trabajo está definido bajo las siguientes consideraciones:

\begin{enumerate}
    \item Se ha adoptado la metodología CRISP-DM como marco de referencia, con una adaptación en su fase final. Dicha fase implica originalmente el despliegue del modelo en un entorno productivo, pero en este trabajo va a enfocarse en la evaluación comparativa de los resultados obtenidos con diferentes algoritmos de clustering, donde se presentarán métricas cuantitativas así como visualizaciones interpretativas de las agrupaciones.
    \item Se llevará a cabo un proceso ETL, el cual obtiene, integra, limpia y normaliza los registros históricos de consumo energético que se tienen de cada cliente, con la finalidad de generar curvas de carga que representen el comportamiento energético de cada cliente. Este proceso contempla la interpolación de valores nulos, la exclusión de días no laborales, corrección de formatos inconsistentes y la normalización mediante técnicas de escalamiento.
    \item Se realizará la optimización e implementación de varios algoritmos de clustering (KMeans, GaussianMixture, Birch y Spectral Clustering), los cuales serán seleccionados en función de su relevancia en la literatura y su aplicabilidad en el análisis de análisis de curvas de carga. Para determinar el número óptimo de agrupaciones se hará uso de métodos de validación como el método del codo. Por otro lado, para la optimización de estos algoritmos se utilizará la correlación intra-cluster, esta métrica es la más adecuada pues captura de mejor manera la similitud en forma de las curvas agrupadas.
    \item Los resultados incluirán la curva de carga representativa de cada cliente, la curva de carga correspondiente al día de máxima demanda, archivos .csv con las coordenadas de dichas curvas. Asimismo, se presentarán resultados visuales de los clústeres y una tabla comparativa con métricas que cuantifican la calidad de las agrupaciones generadas por cada algoritmo.
    \item Para el desarrollo del presente componente se ha contemplado Python como lenguaje de programación de alto nivel, Visual Studio Code como entorno de desarrollo integrado, bibliotecas especializadas en análisis de datos y machine learning (pandas, scikit-learn, numpy, matplotlib, entre otras), así como herramientas de orquestación, en este caso Apache Airflow sobre Docker, para la automatización del proceso ETL.
\end{enumerate}

Por lo anterior expuesto el alcance del componente se limita a la construcción, aplicación y evaluación de modelos de clustering basados en la similitud de curvas de carga, sin abordar fases posteriores como despliegues productivos en entornos de la empresa distribuidora de energía.

% MARCO TEÓRICO

\section{Marco Teórico}

Para comprender este trabajo y su contexto, es de gran importancia tener bases sólidas sobre los principios subyacentes que sustentan el análisis y agrupación de los clientes en función de su curva de carga. Los apartados siguientes explicarán conceptos claves dentro del desarrollo del presente componente.

\subsection{Sobre el sector eléctrico}\\
\begin{enumerate}
\item{\textbf{Clientes no regulados}}\\
Los clientes no regulados en el sector eléctrico son aquellos cuya facturación por el suministro de energía se rige estrictamente por un contrato a término, el cual es realizado entre la empresa que suministra la energía y la empresa que recibe dicha energía. Los contratos mencionados anteriormente son bilaterales\cite{conelec2012}.

Debido a la naturaleza de los contratos que se suscriben con este tipo de clientes, los patrones de consumo de energía que poseen son bastantes variados respecto a los clientes regulados \cite{conelec2012}.


\item{\textbf{Curvas típicas (curva de carga)}}\\
Una curva de carga o también llamada curva típica es un registro gráfico que indica la demanda eléctrica que ha tenido un cliente en cada instante durante un intervalo de tiempo determinado\cite{curva_carga_1}.

Estas curvas de carga reflejan el patrón de consumo cotidiano que poseen los clientes, dicho patrón está directamente relacionado con las máquinas o aparatos que utilizan, así como la energía que consumen durante sus actividades\cite{curva_carga_2}.

\clearpage
\item{\textbf{Segmentación de clientes}}\\
Debido a la naturaleza de los clientes no regulados y, agregando el hecho de que en su mayoría son grandes clientes, segmentarlos en grupos homogéneos permite optimizar la gestión de la demanda y mejorar la planificación del suministro eléctrico. Al agrupar clientes con patrones de consumo similares, es posible diseñar estrategias más eficientes para la contratación de energía, desarrollar y optimizar modelos tarifarios y, mejorar la predicción de la demanda a futuro \cite{curva_carga_1}. Además, esta segmentación ayuda a evitar el sobredimensionamiento o subdimensionamiento de la capacidad de generación y distribución, garantizando un uso más eficiente de los recursos y optimizando los costos operativos.
\end{enumerate}

\subsection{Minería de datos}
Según \cite{datamining-1}, la minería de datos corresponde a un proceso que consiste en la extracción de información relevante a partir de un gran conjunto de datos, con el fin de encontrar patrones interesantes que sean de utilidad, los cuales de otro modo habrían pasado desapercibidos. De la misma manera, métodos tradicionales de análisis de datos son combinados con algoritmos capaces de manejar grandes volúmenes de datos \cite{datamining-2}. Entre sus principales funciones se destacan \cite{datamining-2}:
\begin{enumerate}
\item \textbf{Caracterización/Discriminación:} Sintetizar y explicar clases o conceptos.
\item \textbf{Patrones frecuentes y asociaciones:} Reconocer relaciones que se repiten en el conjunto de datos.
\item \textbf{Clasificación y regresión:} Elaborar modelos para predecir clases o valores numéricos.
\item \textbf{Agrupación:} Generar etiquetas a partir de datos sin clasificar, optimizando la similitud interior.
\item \textbf{Detección de valores atípicos:} Reconocer datos que no se ajustan a un patrón general.
\end{enumerate}

En el contexto de la minería de datos, diversas metodologías han sido propuestas con el fin de dotar de estructura y sistematicidad a este proceso. Estas metodologías proveen fases bien definidas con el fin de asegurar la coherencia entre los objetivos del proyecto y los resultados. A continuación se detallan las tres metodologías más reconocidas en la literatura:

\begin{enumerate}
    \item \textbf{KDD (Knowledge Discovery in Databases)}\\
    KDD fue el primer modelo en recibir aprobación por parte de la comunidad científica para dirigir proyectos cuyo propósito es la obtención de conocimiento a partir de grandes cantidades de datos \cite{moine2011estudio}. Esta metodología plantea un proceso iterativo que incluye la selección de datos, preprocesamiento, transformación, la implementación de algoritmos y el análisis de patrones. Entre sus contribuciones relevantes destaca la distinción de la minería de datos como una fase que forma parte de un proceso más amplio \cite{plotnikova2020adaptations}. A diferencia de otras metodologías posteriores, a KDD se le atribuye un enfoque fundamentalmente conceptual, debido a que establece de manera generalizada cada fase del descubrimiento de conocimiento, sin profundizarlas \cite{moine2011estudio}. KDD es visto como un punto de inicio para la sistematización de la minería de datos debido an esta característica, ya que proporcionó una base para el desarrollo de modelos más integrales que surgieron en años siguientes \cite{moine2011estudio}.
    \item \textbf{CRISP-DM (Cross-Industry Standard Process for Data Mining)}\\
    La metodología CRISP-DM, establecida en el año 2000, ha logrado consolidarse como la más utilizada para proyectos vinculados con la minería de datos \cite{moine2011estudio}. Comprende seis etapas: entendimiento del negocio, entendimiento de los datos, preparación de los datos, modelado, evaluación y despliegue. En este modelo se detalla claramente las tareas y actividades que se deben llevar a cabo en cada fase, lo que permite establecer una conexión entre los objetivos estratégicos y el análisis técnico, es gracias a este equilibrio que CRISP-DM se posiciona como un marco idóneo para proyectos académicos e industriales \cite{plotnikova2020adaptations}. Esta metodología admite retrocesos dentro de su flujo de trabajo, también permite realizar cambios en sus fases en función de los objetivos del proyecto, lo que refuerza su naturaleza iterativa y la hace muy flexible \cite{plotnikova2020adaptations}.
    \item \textbf{SEMMA (Sample, Explore, Modify, Model, Assess)}\\
    SEMMA es un modelo desarrollado por el instituto SAS, establece una guía metodológica que estructura un proceso en cinco fases: muestreo, exploración, modificación, modelado y evaluación \cite{moine2011estudio}. Cada fase está enfocada en los aspectos técnicos del tratamiento de datos y en la aplicación de algoritmos. A diferencia de CRISP-DM, SEMMA no contempla las fases de comprensión del negocio o despliegue, situándola como una metodología muy útil en la ejecución de tareas relativas al análisis de datos, pero ineficaz para tareas donde los objetivos de la organización son clave \cite{plotnikova2020adaptations}. Su utilidad se encuentra en proyectos en los que la experimentación y el modelado son más importantes que integrar el conocimiento adquirido dentro de los procesos comerciales \cite{plotnikova2020adaptations}.
\end{enumerate}

\clearpage
\subsection{Proceso ETL}
El proceso ETL, es una técnica crucial que sirve para obtener, organizar y usar los datos apropiadamente según el fin requerido, se enfoca principalmente en la unión de datos provenientes de diversas fuentes, así como de su evaluación y limpieza \cite{singu2022etl}. Tal y como sus siglas indican, este proceso involucra tres fases descritas a continuación:

\begin{enumerate}
    \item \textbf{Extracción}: Este paso es el responsable de extraer el conjunto requerido de datos de una o más fuentes, donde cada fuente  tiene sus propias características, por lo cual, se debe tener conocimiento sobre como acceder a dichas fuentes, comprender la estructura de las mismas y saber como manejar cada fuente de acuerdo a su naturaleza \cite{elsappagh2011proposed}. Este proceso termina cuando todo el conjunto de datos es consolidado en un solo repositorio \cite{elsappagh2011proposed}.
    \item \textbf{Transformación}: Esta segunda fase consiste en procesar los datos extraídos para que sean consistentes, limpios e integrables dentro del repositorio. Se realizan diversas tareas como reestructurar la información, convertir formatos, limpiar los datos, integrar múltiples fuentes, tratamiento de valores nulos, entre otros \cite{inmon2002building}. El objetivo es asegurar que la información esté depurada y en condiciones para su carga en el repositorio final \cite{inmon2002building}.
    \item \textbf{Carga}: Es la última fase, aquí los datos son almacenados en un repositorio final o en una base de datos para su posterior análisis \cite{gour2010improve}.
\end{enumerate}

\subsection{Aprendizaje no supervisado}
El aprendizaje no supervisado es un tipo de algoritmo de aprendizaje automático, utiliza únicamente datos sin etiquetar, y es usado sobre estos con el objetivo de descubrir patrones o agrupar datos que posiblemente comparten características similares entre sí \cite{Wang2022_unsupervised_machine_learning_urban_studies}. En este contexto, es pertinente destacar algunos elementos clave que permitirán una mejor comprensión, tales como:

\begin{enumerate}
\item\textbf{Clustering}\\
Es una de las categorías del aprendizaje no supervisado, la más consolidada en la actualidad, su objetivo es la identificación de subgrupos dentro de un conjunto extenso de datos no procesados, estos subgrupos son encontrados mediante la diferenciación de características \cite{Wang2022_unsupervised_machine_learning_urban_studies}.
\item\textbf{Número de agrupaciones}\\
Un problema muy común al utilizar algoritmos de aprendizaje no supervisado es elegir el número de agrupaciones deseadas \cite{CoraggioCoretto2021_clusters_quadratic_discriminant_score}, esta elección es muy importante debido a que puede alterar la calidad de las agrupaciones finales dadas por los algoritmos. Como se menciona en \cite{Lezhnina2022_latent_class_cluster_analysis}, esta elección puede ser totalmente subjetiva, y en la mayoría de los casos el números de agrupaciones es seleccionado en función de criterios preestablecidos, sin embargo, existen técnicas como el método del codo que ayudan a validar el número de agrupaciones y que pueden ayudar en la selección de este critero.
\item\textbf{Método del codo}\\
Es la forma más habitual de elegir o validar el número de clústeres, este método consiste en ajustar varios modelos K-means para un rango específico de agrupaciones, normalmente desde 1 hasta un número arbitrario máximo, posteriormente se traza un gráfico que contiene el valor total de la suma de los cuadrados por cada número de clústeres frente a ese respectivo número de clústeres \cite{Friedman2017_survey_R_packages_cluster_analysis}. El objetivo es encontrar aquel valor de número de clústeres donde la gráfica muestra un 'codo' y elegir dicho valor que probablemente nos ofrezca grupos bien separados \cite{Friedman2017_survey_R_packages_cluster_analysis}.

\item\textbf{Algoritmos de clustering}\\
Los algoritmos de clustering son una parte fundamental del aprendizaje no supervisado, pues facilitan el descubrimiento de estructuras y patrones ocultos dentro de un conjunto de datos sin etiquetar \cite{wani2024comprehensive}.

A continuación se describirán los algoritmos de clustering que van a ser utilizados para el desarrollo del presente componente: 

\begin{enumerate}
    \item \textbf{K-Means} \\
    Algoritmo de clustering basado en centroides que organiza $n$ puntos de datos en $k$ clústeres según la proximidad a centroides representativos \cite{wani2024comprehensive}. 
    Cada centroide corresponde a la media de su clúster y el objetivo es minimizar la suma de las distancias al cuadrado entre cada punto y su centroide \cite{wani2024comprehensive}, se puede formular matemáticamente como:
    
    {\setlength{\abovedisplayskip}{-8pt}
    \setlength{\belowdisplayskip}{8pt}\begin{equation}
        J = \sum_{i=1}^{k} \sum_{\mathbf{x}\in S_i} \|\mathbf{x} - \mu_i\|^2 
        \quad \text{con} \quad 
        \mu_i = \frac{1}{|S_i|}\sum_{\mathbf{x}\in S_i} \mathbf{x} 
        \quad \text{y} \quad 
        i = \arg\min_j \|\mathbf{x} - \mu_j\|^2
    \end{equation}}

    \item \textbf{Gaussian Mixture Models (GMM)} \\
    Modelo que asume que los datos provienen de una mezcla de Gaussianas, cada una definida por su media y covarianza \cite{wani2024comprehensive}. 
    Este enfoque permite representar estructuras multimodales donde K-means falla. 
    Los parámetros se estiman con el algoritmo EM, que ajusta iterativamente medias, covarianzas y pesos para representar mejor los datos \cite{wani2024comprehensive}. Matemáticamente, el modelo es expresado como:
    
    {\setlength{\abovedisplayskip}{-8pt}
    \setlength{\belowdisplayskip}{8pt}\begin{equation}
        p(x) = \sum_{j=1}^{k} \pi_j N(x|\mu_j, \Sigma_j), 
        \qquad 
        w_{ij} = \frac{\pi_j N(x_i|\mu_j, \Sigma_j)}
        {\sum_{l=1}^{k} \pi_l N(x_i|\mu_l, \Sigma_l)}
    \end{equation}}
    
    mientras que las actualizaciones de los parámetros en cada iteración están dadas por:

    {\setlength{\abovedisplayskip}{-2pt}
    \setlength{\belowdisplayskip}{8pt}\begin{equation}
        \pi_j = \frac{1}{n}\sum_{i=1}^{n} w_{ij}, 
        \quad 
        \mu_j = \frac{\sum_{i=1}^{n} w_{ij} x_i}{\sum_{i=1}^{n} w_{ij}}, 
        \quad 
        \Sigma_j = \frac{\sum_{i=1}^{n} w_{ij}(x_i-\mu_j)(x_i-\mu_j)^{T}}
        {\sum_{i=1}^{n} w_{ij}}
    \end{equation}}
        

    \item \textbf{Spectral Clustering} \\
    Algoritmo de clustering basado en grafos, transforma los datos en una red donde los nodos representan puntos de datos y las aristas sus similitudes, a partir de esto construye la matriz Laplaciana, cuyos autovectores permiten identificar estructuras dentro del grafo y formar clústeres con alta cohesión interna \cite{wani2024comprehensive}. El objetivo es minimizar la siguiente función:
    
    {\setlength{\abovedisplayskip}{-20pt}
    \setlength{\belowdisplayskip}{8pt}\begin{equation}
        \min \; \text{Tr}(H^{T} L H) \quad \text{sujeto a} \quad H^{T}H = I
    \end{equation}}
        

    \item \textbf{BIRCH} \\
    Es un algoritmo de clustering de tipo jerárquico, está diseñado para trabajar con grandes volúmenes de datos, resumiendo toda la información de los mismos en una sola estructura jerárquica que tiene el nombre de CF-Tree, donde cada clúster es representado como una Clustering Feature (CF) \cite{fontanini2018birch}, la cual está definida por:

    {\setlength{\abovedisplayskip}{1pt}
    \setlength{\belowdisplayskip}{8pt}\begin{equation}
    CF = (N, LS, SS)
    \end{equation}}
    
    donde $N$ es el número de puntos, $LS$ la suma lineal y $SS$ la suma de los cuadrados de los datos. El umbral de radio $T$ se determina mediante un problema de optimización, definido como:

    {\setlength{\abovedisplayskip}{1pt}
    \setlength{\belowdisplayskip}{3pt}\begin{equation}
    \min_T \; g(W_k(T), B_k(T))
    \end{equation}}
    
    donde $W_k$ mide compacidad intra-clúster y $B_k$ separación inter-clúster \cite{fontanini2018birch}.

\end{enumerate}

\clearpage
\item\textbf{Hiperparametrización de algoritmos}\\
Técnica que consiste en ajustar los parámetros que controlan el comportamiento de los algoritmos de clustering, estos parámetros influyen en la calidad de las agrupaciones finales, el objetivo es encontrar aquella combinación de parámetros que ofrezca los mejores resultados en cada algoritmo \cite{pathak2024randomized}. 

\item\textbf{Métricas de evaluación de agrupaciones}\\
Son medidas de calidad que sirven para dar validación a los clústeres obtenidos por los algoritmos, estas métricas se basan en la premisa de 'Maximizar la similitud dentro de cada clúster y minimizar la similitud entre los diferentes clústeres', el objetivo es lograr clústeres compactos y lo más separados posibles entre sí \cite{heras2023tfg}.
\end{enumerate}

\vspace{-0.45cm}

\subsection{Herramientas utilizadas}
Para el desarrollo del componente se han considerado varias herramientas que facilitan las etapas de procesamiento, almacenamiento, análisis de los datos, e implementación de los modelos de clustering, la Tabla \ref{tab:herramientas} los detalla:

\vspace{-0.025cm}

%\begin{footnotesize}
\begin{longtable}{|p{2.5cm}|p{12.3cm}|}
\caption{{{Herramientas utilizadas para el desarrollo del componente}} \label{tab:herramientas}} \\
\hline
\textbf{Herramienta} & \textbf{Descripción de la herramienta} \\
\hline
\endfirsthead

\hline
\textbf{Herramienta} & \textbf{Descripción de la herramienta} \\
\hline
\endhead

\hline
\endfoot

\hline
\endlastfoot

Airflow 2.10.5 & Apache Airflow es una plataforma de código abierto que permite el desarrollo, programación y supervisión flujos de trabajo, utiliza Python, lo que le permite conectarse con diversas tecnologías \cite{airflow_docs}. \\ \hline

Docker 4.43.1 & Docker es una plataforma abierta utilizada para el desarrollo, envío y ejecución de aplicaciones, permite empaquetar y ejecutar aplicaciones en un entorno aislado denominado contenedor \cite{docker_docs}. \\ \hline

Python 3.13 & Python es un lenguaje de programación de alto nivel con naturaleza interpretada, maneja estructuras de datos con un alto nivel de eficiencia y ofrece una sintaxis simple, razones por las cuales es ampliamente utilizado en campos como desarrollo web, ciencia de datos, automatización, entre otros \cite{python_docs}. \\ \hline

Visual Studio Code 1.101.2 & Visual Studio Code es un editor de código fuente que contiene herramientas de depuración, control de versiones y extensiones para varios lenguajes. Ofrece varias características que permiten desarrollar código eficientemente \cite{vscode_docs}.\\ \hline

MongoDB Atlas 8.0.13 & Es una base de datos no relacional administrada en nube, basada en documentos, y que brinda una gran escalabilidad y flexibilidad, además de un modelo avanzado de consultas e indexación \cite{mongodb_docs}.\\
\hline

\end{longtable}
%\end{footnotesize}

% METODOLOGÍA






\chapter{Metodología}
\vspace{-0.5cm}
\section{Caso de estudio}
Unos de los grandes desafíos que enfrenta la Empresa Eléctrica Quito (EEQ) es la administracion eficiente de la demanda de sus clientes, especialmente la de aquel segmento que no está regulado, este grupo de clientes es estratégico debido a su representativo nivel de consumo y a la variablidad de sus patrones de carga. Estos usuarios, no están incluidos en un esquema tarifario regulado, por lo cual exhiben una gran diversidad en sus curvas de demanda. Esto obstaculiza enormemente la planificación energética y el diseño de estrategias destinadas a asegurar eficiencia y fiabilidad en el sistema eléctrico.\\

Los métodos tradicionales que han sido utilizado en la EEQ con el fin de examinar el comportamiento de la demanda (basados principalmente en clasificaciones rígidas o en el cálculo de promedios), han demostrado importantes limitaciones al no conseguir captar la complejidad de los patrones de consumo. En investigaciones anteriores realizadas por Gerencia de Planificación, se ha confirmado esta circunstancia. Los estudios mostraron que el comportamiento energético está directamente relacionado con la actividad económica del cliente no regulado y a su curva de carga característica, lo cual hace inviable que un único criterio generalizado represente apropiadamente a todo este conjunto.\\

\section{Brainstorming}
La lluvia de ideas, también conocida como brainstorming, es un método que se emplea en el campo de la ingeniería de requisitos y la investigación para recopilar información de manera colaborativa. Esto facilita determinar necesidades, cuestiones problemáticas y posibles perspectivas de solución deurante las etapas iniciales de un proyecto. Su valor metodológico radica en su capacidad de permitir reunir un conjunto extenso de percepciones, las cuales pueden ser organizadas y analizadas con más rigor posteriormente, convirtiéndose así en un insumo fundamental para determinación del enfoque metodológico \cite{Weichbroth2016}.\\

En relación con el desarrollo del presente componente, esta técnica se utilizó como método para recopilar información en reuniones con el equipo encargado del departamento de planificación de la demanda. Los métodos de análisis tradicionales, la disparidad de los perfiles de carga y la necesidad de contar con un mecanismo de segmentación que facilite la agrupar a los clientes según su comportamiento energético fueron determinados mediante este proceso.\\

La relevancia del uso de brainstorming en este caso de estudio se explica por el hecho de que, siendo un problema técnico y organizacional complejo, fue imprescindible obtener directamente la experiencia y la sabiduría del personal de la compañía. De este modo, esta técnica hizo posible determinar las necesidades y los problemas más importantes vinculados al estudio del comportamiento energético de los clientes no regulados, lo cual permitió establecer un punto de partida claro para el desarrollo del proyecto.

\section{CRISP-DM}\\
El desarrollo del presente componente se sustenta en CRISP-DM, cuyas siglas corresponden a Cross-Industry Standard Process for Data Mining, metodología que es ampliamente reconocida por su aplicabilidad en proyectos de minería de datos y por brindar un enfoque sistemático y estructurado.\\

La elección de esta metodología se basa en la necesidad de guiar de manera ordenada el análisis del consumo energético de los clientes no regulados del sector eléctrico, y esta opción es la que más se acopla debido a que nos permite avanzar desde la comprensión del problema hasta la obtención de resultados comparables. Además, la metodología CRISP-DM brinda la flexibilidad necesaria para realizar ajustes en sus fases en función de los objetivos que se quieran cumplir, esta característica fue clave para su elección, pues en el presente componente la fase final no contemplará un despliegue productivo como tal, sino una evaluación comparativa de la calidad de agrupaciones obtenidas por cada algoritmo.\\

La validez del uso de CRISP-DM para el desarrollo del presente componente es respaldada por su probado éxito en estudios anteriores similares a este. Sarnovsky y Bednár aplicaron en \cite{sarnovsky2025} esta metodología para realizar clustering de clientes de una empresa distribuidora de energía, donde estos fueron agrupados en función de sus curvas de carga anuales. De manera similar, Otieno adoptó CRISP-DM en \cite{otieno2021} como base para desarrollar diversos análisis de patrones de consumo en una empresa distribuidora de energía. Incluso en otros sectores, como el de software, investigaciones como la presentada en \cite{ijiepr2021} emplean CRISP-DM para estructurar procesos de clustering de clientes basados en técnicas de minería de datos. Estos antecedentes proporcionan una evidencia sólida y específica que valida la elección de CRISP-DM como metodología para el desarrollo del presente componente.\\

CRISP-DM es un método probado utilizado para orientar proyectos de minería de datos. Ofrece una serie de fases que resúmen el ciclo vital de minería de datos, a la vez que incluye descripciones y tareas necesarias en cada fase, ayudando a estructurar un flujo de trabajo ordenado cuya secuencia no es estricta, donde se puede avanzar y retroceder entre fases de ser necesario \cite{crisp-dm1}.\\

El modelo CRISP-DM es sumamente flexible, y sus fases pueden ser personalizadas en función de los objetivos del proyecto, pudiendo crear un modelo de minería de datos que se adapte a necesidades concretas \cite{crisp-dm1}. CRISP-DM contiene un total de seis fases, tal y como se describe en \cite{crisp-dm2}:

\begin{enumerate}
    \item \textbf{Comprensión del negocio:} Esta fase inicial se enfoca en analizar y comprender tanto los objetivos como los requerimientos del proyecto desde la perspectiva del negocio. Posteriormente todo este conocimiento es plasmado en un proyecto de minería de datos enfocado en alcanzar los objetivos.
    
    \item \textbf{Comprensión de los datos:} La fase de comprensión de datos tiene como principal objetivo la 'familiarización' con los datos. Para lograr esto se realiza una recolección inicial de los datos y se procede a realizar un pequeño análisis exploratorio de los datos con el fin de comprender los datos que se tienen e identificar problemas con la calidad de los mismos.
    
    \item \textbf{Preparación de los datos:} Esta fase es crucial en CRISP-DM, debido a que abarca todas las actividades requeridas hasta la construcción final del conjunto de datos, los cuales servirán posteriormente para la fase de modelado. Esta fase incluye tareas como la limpieza, transformación y normalización de los datos, con el fin de asegurar la calidad de estos.
    
    \item \textbf{Modelado:} Varias herramientas de modelamiento son seleccionadas con el fin de ser aplicadas sobre nuestro conjunto de datos preparados. Los parámetros de dichas herramientas deben ser calibrados hasta obtener los valores óptimos que ofrezcan los mejores resultados.
    
    \item \textbf{Evaluación:} En esta penúltima fase del proyecto, ya se tiene construido uno o varios modelos que aparentemente ofrecen resultados de calidad. Antes de proceder a la fase del despliegue, se realiza una evaluación del modelo, revisando cada paso ejecutado hasta la construcción final del mismo con el fin de determinar si existe algún objetivo que no haya sido abordado lo suficiente.
    
    \item \textbf{Despliegue:} La construcción del modelo no es el final del proyecto. En función de los requerimientos, la fase de despliegue puede ser tan simple como la generación de un reporte o tan complejo como su respectiva implementación en otros proyectos de minería de datos.
    
\end{enumerate}

Es importante recalcar que, en el desarrollo del presente componente, la fase número seis de CRISP-DM correspondiente al despliegue fue modificada en función de los objetivos específicos del proyecto. En este caso, a diferencia de la metodología original, que incluye en esta fase la implementación del modelo en un entorno productivo, en este caso, esta fase tendrá un enfoque comparativo de los resultados obtenidos con los distintos algoritmos de clustering. Para ello, las agrupaciones fueron analizadas mediante el uso de métricas de evaluación que permiten cuantificar la calidad de los clústeres, con la finalidad de elegir aquellos resultados que brinden una segmentación más coherente y homogénea. Esta adaptación fue posible gracias a la flexibilidad que caracteriza a CRISP-DM, como se mencionó antes, lo que hace posible la modificación de sus estapas en base a los requerimientos específicos del proyecto, sin perder la consistencia metodoloógica y asegurando la validez del proceso ejecutado.\\

\section{Implementación de CRISP-DM}
\subsection{Flujo de trabajo propuesto}

\begin{itemize}
    \item \textbf{Extracción, Transformación y Carga de los Datos (ETL):}  
    La primera fase consiste en la extracción, transformación y carga (ETL) de los datos de consumo energético. Los datos iniciales provienen de archivos de consumo mensual por cliente. En este paso, se construye un archivo anual para cada cliente, en el cual se agregan y consolidan los datos correspondientes a cada año. Además, los datos son escalados y normalizados para garantizar su consistencia y comparabilidad. Este proceso se lleva a cabo con la ayuda de \textbf{Apache Airflow}, el cual permite automatizar el flujo de trabajo y garantizar su ejecución eficiente. Finalmente, los datos transformados son cargados en \textbf{MongoDB}, asegurando su disponibilidad para las fases siguientes del análisis.

    \item \textbf{Segmentación de Clientes:}  
    En esta fase, se procede a definir el número óptimo de grupos de clientes a través de un análisis conversacional con las partes interesadas y la aplicación de métodos como el \textbf{método del codo}. Una vez definido el número de grupos, se seleccionan y optimizan los algoritmos de agrupación más adecuados para el análisis, tales como \textbf{KMeans}, \textbf{GaussianMixture}, \textbf{Birch} y \textbf{Spectral Clustering}. Estos algoritmos se utilizan para agrupar a los clientes según la similitud de sus curvas de consumo energético anual. Los resultados obtenidos se presentan visualmente, permitiendo observar las agrupaciones y patrones emergentes en el consumo de energía de los clientes.

    \item \textbf{Evaluación Comparativa de los Algoritmos:}  
    Finalmente, se realiza una evaluación comparativa de los algoritmos de agrupación aplicados, utilizando diversas métricas para medir la calidad de las agrupaciones. Entre las métricas utilizadas se encuentran el \textbf{Silhouette Score}, \textbf{SSE (Suma de Errores al Cuadrado)}, \textbf{DBI (Índice de la Diferencia de Davies-Bouldin)} y \textbf{CHI (Índice de Calinski-Harabasz)}. Estas métricas permiten analizar el rendimiento de los algoritmos y seleccionar el que mejor se adapte a los datos de consumo energético de los clientes.
\end{itemize}


\begin{enumerate}
     

\item \textbf{Proceso ETL}\\
El primer proceso consiste en el desarrollo de un flujo ETL bajo el marco de trabajo de Apache Airflow, este proceso nos permitirá estructurar y preparar los datos de consumo de los clientes en curvas anuales características para su posterior análisis de agrupación. La \autoref{fig:fig1} ilustra de manera detallada todas las etapas que abarca este proceso.\\

\renewcommand{\thefigure}{\arabic{figure}}

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.7]{./imgs/ETL_process.png}
\caption{{Proceso ETL con sus etapas}}
\label{fig:fig1}
\end{center}
\end{figure}

\item \textbf{Proceso de agrupación}\\
El segundo proceso comprende todo el proceso de agrupamiento, en este apartado se elegirá el número de agrupaciones deseadas, se seleccionarán y aplicarán diversos algoritmos de agrupamiento para finalmente evaluar la calidad de las agrupaciones obtenidas por cada algoritmo. La \autoref{fig:fig2} ilustra de manera detallada todas las etapas que abarca este proceso.\\

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=0.7]{./imgs/Clustering_process.png}
\caption{{Proceso de agrupación con sus etapas}}
\label{fig:fig2}
\end{center}
\end{figure}

\end{enumerate}

Los datos utilizados para el presente componente comprenden todas las mediciones mensuales del año 2024 por cada cliente, las cuales han sido obtenidos de la página de telemediciones de la Empresa Eléctrica de Quito.\\
    
Por otro lado, la segmentación de clientes se realiza exclusivamente en función de la forma de su curva característica anual, obtenida al final del proceso ETL descrito en la \autoref{fig:fig1}. No se consideran otros factores, como las tarifas o la geolocalización, ya que el objetivo de la parte interesada es agrupar a los clientes estrictamente según el patrón de consumo de energía reflejado en su curva característica anual.\\

\textbf{PARRAFOS HUERFANOS, INCLUIR TALVEZ EN CRISP DM}
En este contexto, es indispensable emplear un enfoque innovador que facilite la identificación de patrones de consumo representativos y que haga posible la segmentación de los clientes no regulados en función de su comportamiento energético real. Con este propósito, se plantea el presente componente que, a través de técnicas de aprendizaje no supervisado, produzca conjuntos homogéneos de clientes basándose en la forma de sus curvas anuales de carga.\\

Esta solución proporciona un valor agregado debido a que permitirá a la EEQ comprender de mejor manera la diversidad de comportamientos respecto al consumo de los clientes no regulados, definir criterios técnicos más firmes para el manejo de la demanda y planificación de la red, y mejorar el proceso de toma de decisiones estratégicas relacionadas con las tarifas, políticas energéticas y expansión de infraestructura. En este sentido, el caso de estudio se basa en la necesidad de superar las limitaciones de métodos convencionales y avanzar hacia un proceso inteligente de segmentación, que se convierta en un recurso esencial para la planificación energética en la zona cubierta por la EEQ.\\

% RESULTADOS, CONCLUSIONES Y RECOMENDACIONES


\chapter{Resultados, Conclusiones y Recomendaciones}
\section{Resultados}
Ejemplo de tabla:
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        No. Prueba & Resultado & Tiempo [s] \\
        \hline
        1 & 10 & 0.9 \\
        2 & 5 & 0.5 \\
        \hline
    \end{tabular}
    \caption{Resultados de las pruebas realizadas}
\end{table}

\section{Conclusiones}
\section{Recomendaciones}


% REFERENCIAS BIBLIOGRÁFICAS
\chapter{Referencias Bibliográficas}

\printbibliography[heading=none]

Ejemplo IEEE:
\begin{itemize}
    \item \textbf{[1]} L. Carvajal, \textit{Metodología de la Investigación Científica}. Santiago de Cali: U.S.C., 2006.
\end{itemize}



https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/ccs2.12080\\

https://hpi.de/fileadmin/user_upload/fachgebiete/naumann/publications/2008/ETL_Management.pdf\\

https://www.researchgate.net/profile/Sameer_Shukla3/publication/369899578_Developing_Pragmatic_Data_Pipelines_using_Apache_Airflow_on_Google_Cloud_Platform/links/647953c7b3dfd73b7759022a/Developing-Pragmatic-Data-Pipelines-using-Apache-Airflow-on-Google-Cloud-Platform.pdf?origin=journalDetail&_tp=eyJwYWdlIjoiam91cm5hbERldGFpbCJ9\\

https://www.controlrecursosyenergia.gob.ec/wp-content/uploads/downloads/2021/03/Folleto-Resumen-Estad%C3%ADsticas-2011.pdf\\



\chapter{Anexos}
\section*{Anexo I. Conjunto de Datos Extensos}
\section*{Anexo II. Formato de Entrevista}
\section*{Anexo III. Enlaces}

\end{document}